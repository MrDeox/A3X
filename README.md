# AÂ³X - Sistema de InteligÃªncia Artificial Local

Sistema modular de inteligÃªncia artificial local com suporte a GPU AMD via ROCm.

## ğŸš€ CaracterÃ­sticas

- ExecuÃ§Ã£o local de modelos GGUF
- Suporte a GPU AMD via ROCm
- Interface Python simples e modular
- Baseado no llama.cpp

## ğŸ› ï¸ Requisitos

- Python 3.8+
- GPU AMD com suporte a ROCm
- llama.cpp compilado com suporte a ROCm

## ğŸ“¦ InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/MrDeox/A3X.git
cd A3X
```

2. Compile o llama.cpp com suporte a ROCm:
```bash
cd llama.cpp
mkdir build && cd build
cmake .. -DLLAMA_HIPBLAS=ON
cmake --build . --config Release
cd ../..
```

3. Baixe o modelo (opcional, se nÃ£o tiver):
```bash
# O modelo serÃ¡ baixado automaticamente na primeira execuÃ§Ã£o
```

## ğŸ’¡ Uso

```python
from llm.inference import run_llm

# Exemplo simples
resposta = run_llm("Qual Ã© a sua missÃ£o?")
print(resposta)

# Com nÃºmero mÃ¡ximo de tokens personalizado
resposta = run_llm("Explique o que Ã© inteligÃªncia artificial", max_tokens=256)
print(resposta)
```

## ğŸ—ï¸ Estrutura do Projeto

```
A3X/
â”œâ”€â”€ llm/                    # MÃ³dulo Python para inferÃªncia
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ inference.py
â”œâ”€â”€ llama.cpp/             # SubmÃ³dulo llama.cpp
â”œâ”€â”€ models/                # DiretÃ³rio para modelos GGUF
â””â”€â”€ README.md
```

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor, leia o [CONTRIBUTING.md](CONTRIBUTING.md) para detalhes sobre nosso cÃ³digo de conduta e o processo para enviar pull requests. 