# Example script to test CognitiveGraph inference using pre-trained fragments.

import torch

# Updated import paths after moving a3net into a3x
from a3x.a3net.core.cognitive_graph import CognitiveGraph
from a3x.a3net.integration.a3x_bridge import MEMORY_BANK 

if __name__ == '__main__':
    print("--- Running A³Net Cognitive Graph Inference Example ---")

    # --- Access the Shared Memory Bank ---
    # MEMORY_BANK is imported directly, assuming it was populated by a previous run
    # (e.g., by running test_directive_training.py)
    print(f"\nAccessing shared MemoryBank. Currently contains {len(MEMORY_BANK)} fragments.")
    print(f"Saved fragment IDs: {MEMORY_BANK.list()}")

    # --- Define Fragment Sequence ---
    # Use the ID(s) of the fragment(s) you expect to be in the MemoryBank.
    # The ID generated by test_directive_training.py based on its goal is likely:
    fragment_id_from_training = "frag_example_training_with_neura" 
    
    # Define the sequence for the graph
    graph_fragment_ids = [fragment_id_from_training]
    # If you had multiple trained fragments, you could list them here:
    # graph_fragment_ids = ["frag_first_step", "frag_second_step", frag_id_from_training]
    
    print(f"\nAttempting to build CognitiveGraph with fragment IDs: {graph_fragment_ids}")

    # --- Create Cognitive Graph ---
    # The graph will load the specified fragments from the MEMORY_BANK
    cognitive_graph = CognitiveGraph(memory_bank=MEMORY_BANK, fragment_ids=graph_fragment_ids)

    # Check if the graph loaded any fragments
    if len(cognitive_graph.fragments) == 0:
        print("\nError: CognitiveGraph failed to load any fragments. Aborting inference.")
    else:
        # --- Prepare Input Data ---
        # Create a dummy input tensor matching the expected input dimension (e.g., 128)
        # The shape should be [batch_size, input_dim]
        input_tensor = torch.randn(1, 128) 
        print(f"\nPrepared input tensor with shape: {input_tensor.shape}")

        # --- Run Inference ---
        print("\nRunning inference through CognitiveGraph...")
        # The forward pass uses torch.no_grad() internally
        output_tensor = cognitive_graph(input_tensor)
        
        print(f"\nInference output tensor:\n{output_tensor}")
        print(f"Output tensor shape: {output_tensor.shape}")

    print("\n--- A³Net Cognitive Graph Inference Example Finished ---") 